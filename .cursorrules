# ì›¹ í¬ë¡¤ë§ & ìë™í™” ì™„ë²½ ê°€ì´ë“œ

> ì´ ê°€ì´ë“œëŠ” Pythonì„ ì‚¬ìš©í•œ ì›¹ í¬ë¡¤ë§ê³¼ ìë™í™”ì— ëŒ€í•œ í¬ê´„ì ì¸ ì•ˆë‚´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“š ëª©ì°¨

1. [ê°œìš” ë° ì¤€ë¹„](#1-ê°œìš”-ë°-ì¤€ë¹„)
2. [ê¸°ë³¸ í¬ë¡¤ë§ ì„¤ì •](#2-ê¸°ë³¸-í¬ë¡¤ë§-ì„¤ì •)
3. [ë´‡ ê°ì§€ ìš°íšŒ ê¸°ë²•](#3-ë´‡-ê°ì§€-ìš°íšŒ-ê¸°ë²•)
4. [ê³ ê¸‰ í¬ë¡¤ë§ ê¸°ìˆ ](#4-ê³ ê¸‰-í¬ë¡¤ë§-ê¸°ìˆ )
5. [ë°ì´í„° ì²˜ë¦¬ ë° ì €ì¥](#5-ë°ì´í„°-ì²˜ë¦¬-ë°-ì €ì¥)
6. [ì„±ëŠ¥ ìµœì í™”](#6-ì„±ëŠ¥-ìµœì í™”)
7. [ëª¨ë²” ì‚¬ë¡€ ë° ì£¼ì˜ì‚¬í•­](#7-ëª¨ë²”-ì‚¬ë¡€-ë°-ì£¼ì˜ì‚¬í•­)

## 1. ê°œìš” ë° ì¤€ë¹„

### 1.1 í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

```bash
# ê¸°ë³¸ í¬ë¡¤ë§ ë„êµ¬
pip install requests beautifulsoup4
pip install selenium webdriver_manager

# ë´‡ ê°ì§€ ìš°íšŒ ë„êµ¬
pip install fake-useragent
pip install undetected-chromedriver
pip install python-anticaptcha

# ì„±ëŠ¥ ìµœì í™” ë„êµ¬
pip install aiohttp
pip install redis celery
```

### 1.2 ê¸°ë³¸ ì„¤ì •

```python
import random
import time
from fake_useragent import UserAgent
import undetected_chromedriver as uc
from selenium.webdriver.chrome.options import Options

def setup_base_configuration():
    ua = UserAgent()
    
    chrome_options = Options()
    chrome_options.add_argument(f'user-agent={ua.random}')
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    
    return chrome_options
```

## 2. ê¸°ë³¸ í¬ë¡¤ë§ ì„¤ì •

### 2.1 í—¤ë” ë° ì„¸ì…˜ ê´€ë¦¬

```python
def create_headers():
    ua = UserAgent()
    return {
        'User-Agent': ua.random,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'DNT': '1',
        'Upgrade-Insecure-Requests': '1'
    }

class SessionManager:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers = create_headers()
        self.session.cookies.clear()

    def rotate_session(self):
        self.session.close()
        self.session = requests.Session()
        self.session.headers = create_headers()
```

### 2.2 ê¸°ë³¸ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤

```python
class BaseCrawler:
    def __init__(self):
        self.session_manager = SessionManager()
        self.driver = self._setup_driver()

    def _setup_driver(self):
        options = setup_base_configuration()
        return uc.Chrome(options=options)

    def get_page(self, url, use_selenium=False):
        if use_selenium:
            self.driver.get(url)
            return self.driver.page_source
        else:
            response = self.session_manager.session.get(url)
            return response.text
```

## 3. ë´‡ ê°ì§€ ìš°íšŒ ê¸°ë²•

### 3.1 ë¸Œë¼ìš°ì € ì§€ë¬¸ ë¬´ì‘ìœ„í™”

```python
class BrowserFingerprint:
    @staticmethod
    def apply_random_fingerprint(driver):
        # WebGL ë Œë”ëŸ¬ ì •ë³´ ìˆ˜ì •
        driver.execute_script("""
            const getParameter = WebGLRenderingContext.prototype.getParameter;
            WebGLRenderingContext.prototype.getParameter = function(parameter) {
                if (parameter === 37445) {
                    return 'Intel Open Source Technology Center';
                }
                if (parameter === 37446) {
                    return 'Mesa DRI Intel(R) Sandybridge Mobile';
                }
                return getParameter.apply(this, arguments);
            };
        """)

        # í™”ë©´ í•´ìƒë„ ë¬´ì‘ìœ„í™”
        driver.execute_script(f"""
            Object.defineProperty(window.screen, 'width', {{
                get: function() {{ return {random.choice([1366, 1440, 1920])}; }}
            }});
            Object.defineProperty(window.screen, 'height', {{
                get: function() {{ return {random.choice([768, 900, 1080])}; }}
            }});
        """)
```

### 3.2 ì¸ê°„ë‹¤ìš´ í–‰ë™ ì‹œë®¬ë ˆì´ì…˜

```python
class HumanBehaviorSimulator:
    def __init__(self, driver):
        self.driver = driver
        
    def random_scroll(self):
        """ìì—°ìŠ¤ëŸ¬ìš´ ìŠ¤í¬ë¡¤ ë™ì‘ ìˆ˜í–‰"""
        scroll_amount = random.randint(100, 500)
        self.driver.execute_script(f"window.scrollBy(0, {scroll_amount});")
        time.sleep(random.uniform(0.5, 1.5))
        
    def random_mouse_movement(self):
        """ìì—°ìŠ¤ëŸ¬ìš´ ë§ˆìš°ìŠ¤ ì›€ì§ì„ ì‹œë®¬ë ˆì´ì…˜"""
        actions = ActionChains(self.driver)
        x_offset = random.randint(-100, 100)
        y_offset = random.randint(-100, 100)
        actions.move_by_offset(x_offset, y_offset)
        actions.perform()
        time.sleep(random.uniform(0.1, 0.3))
        
    def type_like_human(self, element, text):
        """ì¸ê°„ë‹¤ìš´ íƒ€ì´í•‘ ì‹œë®¬ë ˆì´ì…˜"""
        for char in text:
            element.send_keys(char)
            time.sleep(random.uniform(0.1, 0.3))
```

### 3.3 ë„¤íŠ¸ì›Œí¬ íŒ¨í„´ ë‹¤ì–‘í™”

```python
class NetworkPatternRandomizer:
    def __init__(self):
        self.proxy_pool = self._load_proxy_pool()
        
    def _load_proxy_pool(self):
        # í”„ë¡ì‹œ ëª©ë¡ ë¡œë“œ (ì˜ˆì‹œ)
        return [
            'http://proxy1.example.com:8080',
            'http://proxy2.example.com:8080',
            'http://proxy3.example.com:8080'
        ]
        
    def get_random_proxy(self):
        return random.choice(self.proxy_pool)
        
    def random_delay(self):
        time.sleep(random.uniform(2, 5))
```

## 4. ê³ ê¸‰ í¬ë¡¤ë§ ê¸°ìˆ 

### 4.1 ë™ì  ì½˜í…ì¸  ì²˜ë¦¬

```python
class DynamicContentHandler:
    def __init__(self, driver):
        self.driver = driver
        
    def handle_infinite_scroll(self):
        """ë¬´í•œ ìŠ¤í¬ë¡¤ ì²˜ë¦¬"""
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        
        while True:
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(random.uniform(1, 2))
            
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
                
            last_height = new_height
            
    def wait_for_dynamic_content(self, selector, timeout=10):
        """ë™ì ìœ¼ë¡œ ë¡œë“œë˜ëŠ” ì»¨í…ì¸  ëŒ€ê¸°"""
        try:
            element = WebDriverWait(self.driver, timeout).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, selector))
            )
            return element
        except TimeoutException:
            return None
```

### 4.2 ìº¡ì°¨(CAPTCHA) ì²˜ë¦¬

```python
class CaptchaHandler:
    def __init__(self, api_key):
        self.solver = AnticaptchaClient(api_key)
        
    def solve_recaptcha(self, site_key, url):
        task = NoCaptchaTaskProxyless(url, site_key)
        job = self.solver.createTask(task)
        job.join()
        return job.get_solution_response()
        
    def solve_image_captcha(self, image_path):
        with open(image_path, 'rb') as image_file:
            task = ImageToTextTask(image_file)
            job = self.solver.createTask(task)
            job.join()
            return job.get_captcha_text()
```

## 5. ë°ì´í„° ì²˜ë¦¬ ë° ì €ì¥

### 5.1 ë°ì´í„° ì •ì œ

```python
class DataCleaner:
    @staticmethod
    def clean_text(text):
        """í…ìŠ¤íŠ¸ ë°ì´í„° ì •ì œ"""
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        text = ' '.join(text.split())
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬
        text = re.sub(r'[^\w\s]', ' ', text)
        
        return text.strip()
        
    @staticmethod
    def validate_data(item):
        """ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬"""
        required_fields = ['title', 'content', 'date']
        return all(field in item and item[field] for field in required_fields)
```

### 5.2 ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥

```python
class DatabaseHandler:
    def __init__(self):
        self.client = MongoClient('mongodb://localhost:27017/')
        self.db = self.client['crawler_db']
        
    def save_item(self, item):
        """ë°ì´í„° ì €ì¥"""
        if not DataCleaner.validate_data(item):
            return False
            
        collection = self.db['crawled_data']
        try:
            collection.insert_one(item)
            return True
        except Exception as e:
            print(f"ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return False
```

## 6. ì„±ëŠ¥ ìµœì í™”

### 6.1 ë¹„ë™ê¸° í¬ë¡¤ë§

```python
class AsyncCrawler:
    def __init__(self, max_concurrency=5):
        self.semaphore = asyncio.Semaphore(max_concurrency)
        
    async def crawl_url(self, url, session):
        async with self.semaphore:
            try:
                async with session.get(url) as response:
                    return await response.text()
            except Exception as e:
                print(f"Error crawling {url}: {str(e)}")
                return None
                
    async def crawl_multiple_urls(self, urls):
        async with aiohttp.ClientSession() as session:
            tasks = [self.crawl_url(url, session) for url in urls]
            return await asyncio.gather(*tasks)
```

### 6.2 ë¶„ì‚° ì²˜ë¦¬

```python
class DistributedCrawler:
    def __init__(self):
        self.celery = Celery('crawler', broker='redis://localhost:6379/0')
        
    @self.celery.task
    def crawl_task(self, url):
        crawler = BaseCrawler()
        return crawler.get_page(url)
        
    def start_distributed_crawling(self, urls):
        return [self.crawl_task.delay(url) for url in urls]
```

## 7. ëª¨ë²” ì‚¬ë¡€ ë° ì£¼ì˜ì‚¬í•­

### 7.1 ë¦¬ì†ŒìŠ¤ ê´€ë¦¬

```python
class ResourceManager:
    def __init__(self):
        self.active_connections = []
        
    def add_connection(self, connection):
        self.active_connections.append(connection)
        
    def cleanup(self):
        for connection in self.active_connections:
            try:
                connection.close()
            except Exception as e:
                print(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")
```

### 7.2 ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë³µêµ¬

```python
class ErrorHandler:
    @staticmethod
    def handle_connection_error(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except requests.exceptions.RequestException as e:
                    print(f"ì—°ê²° ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {str(e)}")
                    if attempt < max_retries - 1:
                        time.sleep(random.uniform(1, 3))
                    else:
                        raise
        return wrapper
```

### ì‹¤í–‰ ì˜ˆì œ

```python
if __name__ == "__main__":
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = BaseCrawler()
    simulator = HumanBehaviorSimulator(crawler.driver)
    data_handler = DatabaseHandler()
    
    try:
        # í¬ë¡¤ë§ ìˆ˜í–‰
        urls = ["http://example.com/page1", "http://example.com/page2"]
        for url in urls:
            # ë´‡ ê°ì§€ ìš°íšŒ ì„¤ì •
            self.anti_detection = AdvancedAntiDetection(driver)
            self.anti_detection.bypass_selenium_detection()
            
            # ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ ì œì–´
            network_controller = NetworkTrafficController(driver)
            network_controller.set_request_interceptor()
            network_controller.modify_headers()
            
            results = []
            for url in urls:
                try:
                    # ì¸ê°„ë‹¤ìš´ í–‰ë™ ì‹œë®¬ë ˆì´ì…˜
                    self.anti_detection.randomize_mouse_movements()
                    await asyncio.sleep(self.anti_detection.emulate_human_timing())
                    
                    # í˜ì´ì§€ í¬ë¡¤ë§
                    start_time = time.time()
                    content = await self.crawl_page(driver, url)
                    response_time = time.time() - start_time
                    
                    if content:
                        # ë°ì´í„° ì²˜ë¦¬
                        cleaned_data = self.process_data(content)
                        results.append(cleaned_data)
                        
                        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡
                        self.monitor.record_metric('crawling_task', success=True, response_time=response_time)
                    else:
                        self.monitor.record_metric('crawling_task', success=False)
                        
                except Exception as e:
                    self.logger.logger.error(f"Error crawling {url}: {str(e)}")
                    self.monitor.record_metric('crawling_task', success=False)
                    
            return results
            
        finally:
            driver.quit()
            
    async def crawl_page(self, driver, url):
        """ë‹¨ì¼ í˜ì´ì§€ í¬ë¡¤ë§"""
        try:
            driver.get(url)
            await asyncio.sleep(2)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            
            # ë™ì  ì»¨í…ì¸  ì²˜ë¦¬
            dynamic_handler = DynamicContentHandler(driver)
            dynamic_handler.handle_infinite_scroll()
            
            return driver.page_source
            
        except Exception as e:
            self.logger.logger.error(f"Error in crawl_page: {str(e)}")
            return None
            
    def process_data(self, content):
        """ë°ì´í„° ì²˜ë¦¬ ë° ì •ì œ"""
        cleaner = DataCleaner()
        validator = DataValidator()
        
        # ë°ì´í„° ì •ì œ
        cleaned_content = cleaner.clean_html(content)
        cleaned_content = cleaner.normalize_whitespace(cleaned_content)
        cleaned_content = cleaner.remove_special_chars(cleaned_content)
        
        # ë°ì´í„° ê²€ì¦
        data = {'content': cleaned_content}
        is_valid, errors = validator.validate_data(data)
        
        if not is_valid:
            self.logger.logger.warning(f"Data validation errors: {errors}")
            
        return data
```

## 11. ì‹¤ì „ ì‘ìš© ì‹œë‚˜ë¦¬ì˜¤

### 11.1 ëŒ€ê·œëª¨ ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ

```python
class LargeScaleCrawler:
    def __init__(self):
        self.distributed_crawler = DistributedCrawler()
        self.stream_processor = StreamProcessor()
        self.parallel_processor = ParallelProcessor()
        
    async def run_large_scale_crawl(self, url_list):
        """ëŒ€ê·œëª¨ í¬ë¡¤ë§ ì‘ì—… ì‹¤í–‰"""
        # URL ì²­í¬ë¡œ ë¶„í• 
        url_chunks = [url_list[i:i+1000] for i in range(0, len(url_list), 1000)]
        
        results = []
        for chunk in url_chunks:
            # ë¶„ì‚° í¬ë¡¤ë§ ì‘ì—… ì‹œì‘
            crawl_tasks = self.distributed_crawler.start_distributed_crawling(chunk)
            
            # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë°ì´í„° ì²˜ë¦¬
            async for data in self.stream_processor.process_stream(crawl_tasks, self.process_data):
                if data:
                    results.extend(data)
                    
        return results
        
    def process_data(self, data_chunk):
        """ë³‘ë ¬ ë°ì´í„° ì²˜ë¦¬"""
        return self.parallel_processor.process_data_parallel(
            data_chunk,
            self._process_single_item
        )
        
    def _process_single_item(self, item):
        """ë‹¨ì¼ ë°ì´í„° ì•„ì´í…œ ì²˜ë¦¬"""
        try:
            # ë°ì´í„° ì •ì œ ë° ê²€ì¦
            cleaner = DataCleaner()
            cleaned_item = {
                'content': cleaner.clean_html(item.get('content', '')),
                'timestamp': datetime.now()
            }
            
            return cleaned_item
            
        except Exception as e:
            print(f"ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return None
```

### 11.2 ì§€ëŠ¥í˜• í¬ë¡¤ë§ ì‹œìŠ¤í…œ

```python
class IntelligentCrawler:
    def __init__(self):
        self.url_patterns = {}
        self.content_analyzer = None
        
    def learn_patterns(self, training_data):
        """íŒ¨í„´ í•™ìŠµ"""
        for url, content in training_data:
            # URL íŒ¨í„´ ë¶„ì„
            pattern = self._extract_url_pattern(url)
            
            # ì½˜í…ì¸  íŒ¨í„´ ë¶„ì„
            content_pattern = self._analyze_content_structure(content)
            
            self.url_patterns[pattern] = content_pattern
            
    def _extract_url_pattern(self, url):
        """URL íŒ¨í„´ ì¶”ì¶œ"""
        # URL êµ¬ì¡° ë¶„ì„
        parsed_url = urlparse(url)
        path_parts = parsed_url.path.split('/')
        
        # íŒ¨í„´ ìƒì„±
        pattern_parts = []
        for part in path_parts:
            if part.isdigit():
                pattern_parts.append('{\d+}')
            else:
                pattern_parts.append(part)
                
        return '/'.join(pattern_parts)
        
    def _analyze_content_structure(self, content):
        """ì½˜í…ì¸  êµ¬ì¡° ë¶„ì„"""
        soup = BeautifulSoup(content, 'html.parser')
        structure = {}
        
        # ì£¼ìš” ì½˜í…ì¸  ì˜ì—­ ì‹ë³„
        for element in soup.find_all(['div', 'article', 'section']):
            if element.get('class'):
                structure[str(element.get('class'))] = {
                    'tag': element.name,
                    'attributes': element.attrs,
                    'content_type': self._identify_content_type(element)
                }
                
        return structure
        
    def _identify_content_type(self, element):
        """ì½˜í…ì¸  íƒ€ì… ì‹ë³„"""
        # í…ìŠ¤íŠ¸ ë¹„ìœ¨ ê³„ì‚°
        text_ratio = len(element.get_text(strip=True)) / len(str(element))
        
        # ì´ë¯¸ì§€ ìˆ˜ ê³„ì‚°
        image_count = len(element.find_all('img'))
        
        if text_ratio > 0.7:
            return 'text'
        elif image_count > 0:
            return 'mixed'
        else:
            return 'unknown'
```

## 12. ë³´ì•ˆ ë° ìš°íšŒ ê¸°ìˆ  ê³ ë„í™”

### 12.1 ê³ ê¸‰ ë¸Œë¼ìš°ì € ìë™í™” ìš°íšŒ

```python
class AdvancedBrowserAutomation:
    def __init__(self):
        self.driver = None
        self.stealth_patches = []
        
    def apply_stealth_patch(self):
        """ìŠ¤í…”ìŠ¤ íŒ¨ì¹˜ ì ìš©"""
        patches = [
            self._patch_navigator(),
            self._patch_webdriver(),
            self._patch_chrome_runtime(),
            self._patch_permissions()
        ]
        
        for patch in patches:
            self.driver.execute_script(patch)
            
    def _patch_navigator(self):
        """Navigator ê°ì²´ íŒ¨ì¹˜"""
        return """
        const originalNavigator = window.navigator;
        const navigator_properties = {};
        
        for (const prop in originalNavigator) {
            if (prop !== 'webdriver') {
                navigator_properties[prop] = {
                    get: () => originalNavigator[prop]
                };
            }
        }
        
        navigator_properties['webdriver'] = {
            get: () => undefined
        };
        
        Object.defineProperties(
            window.navigator,
            navigator_properties
        );
        """
        
    def _patch_permissions(self):
        """Permissions API íŒ¨ì¹˜"""
        return """
        const originalQuery = window.navigator.permissions.query;
        window.navigator.permissions.query = (parameters) => (
            parameters.name === 'notifications' ?
            Promise.resolve({ state: Notification.permission }) :
            originalQuery(parameters)
        );
        """
```

### 12.2 ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ ë§ˆìŠ¤í‚¹

```python
class NetworkTrafficMasker:
    def __init__(self):
        self.proxy_rotator = ProxyRotator()
        self.traffic_patterns = []
        
    def add_traffic_pattern(self, pattern):
        """íŠ¸ë˜í”½ íŒ¨í„´ ì¶”ê°€"""
        self.traffic_patterns.append(pattern)
        
    def generate_natural_traffic(self):
        """ìì—°ìŠ¤ëŸ¬ìš´ íŠ¸ë˜í”½ ìƒì„±"""
        pattern = random.choice(self.traffic_patterns)
        
        # íŒ¨í„´ì— ë”°ë¥¸ ìš”ì²­ ìƒì„±
        requests = []
        for step in pattern:
            request = {
                'url': step['url'],
                'method': step['method'],
                'headers': self._generate_headers(),
                'timing': self._calculate_timing()
            }
            requests.append(request)
            
        return requests
        
    def _generate_headers(self):
        """ë™ì  í—¤ë” ìƒì„±"""
        return {
            'User-Agent': self._get_random_user_agent(),
            'Accept-Language': self._get_random_language(),
            'Accept': self._get_random_accept_header(),
            'Connection': random.choice(['keep-alive', 'close']),
            'Cache-Control': random.choice(['no-cache', 'max-age=0'])
        }
        
    def _calculate_timing(self):
        """ìì—°ìŠ¤ëŸ¬ìš´ íƒ€ì´ë° ê³„ì‚°"""
        # ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ì‚¬ìš©í•œ ì§€ì—° ì‹œê°„ ê³„ì‚°
        base_delay = random.gauss(2, 0.5)
        jitter = random.uniform(-0.1, 0.1)
        return max(0.1, base_delay + jitter)
```

## 13. ìµœì¢… ì‹¤í–‰ ì˜ˆì œ

```python
async def main():
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    crawler = IntegratedCrawlingSystem()
    logger = CrawlerLogger('MainSystem')
    monitor = PerformanceMonitor()
    
    try:
        # í¬ë¡¤ë§ ëŒ€ìƒ URL ëª©ë¡
        urls = [
            "https://example.com/page1",
            "https://example.com/page2",
            "https://example.com/page3"
        ]
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        monitor.start_monitoring('main_task')
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        results = await crawler.run_crawling_task(urls)
        
        # ê²°ê³¼ ì²˜ë¦¬
        if results:
            logger.logger.info(f"Successfully crawled {len(results)} pages")
            
            # ê²°ê³¼ ì €ì¥
            save_results(results)
            
        # ì„±ëŠ¥ í†µê³„ ì¶œë ¥
        stats = monitor.get_statistics('main_task')
        logger.logger.info(f"Performance statistics: {stats}")
        
    except Exception as e:
        logger.logger.error(f"Main task failed: {str(e)}")
        
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        cleanup_resources()

if __name__ == "__main__":
    asyncio.run(main())
```

## ì£¼ì˜ì‚¬í•­ ë° ëª¨ë²” ì‚¬ë¡€

1. **ì›¹ì‚¬ì´íŠ¸ ë¦¬ì†ŒìŠ¤ ì¡´ì¤‘**
   - robots.txt ê·œì¹™ ì¤€ìˆ˜
   - ì ì ˆí•œ ìš”ì²­ ê°„ê²© ìœ ì§€
   - ê³¼ë„í•œ ë¶€í•˜ ë°©ì§€

2. **ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë³µêµ¬**
   - ëª¨ë“  ì˜ˆì™¸ ìƒí™© ì²˜ë¦¬
   - ìë™ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
   - ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§

3. **ì„±ëŠ¥ ìµœì í™”**
   - ë¹„ë™ê¸° ì‘ì—… í™œìš©
   - ë¦¬ì†ŒìŠ¤ ì‚¬ìš© ìµœì í™”
   - ìºì‹± ì „ëµ êµ¬í˜„

4. **ë³´ì•ˆ ê³ ë ¤ì‚¬í•­**
   - IP ì°¨ë‹¨ ë°©ì§€
   - ë´‡ ê°ì§€ ìš°íšŒ
   - ë°ì´í„° ë³´ì•ˆ ìœ ì§€

ì´ ê°€ì´ë“œë¥¼ í†µí•´ ì•ˆì •ì ì´ê³  íš¨ìœ¨ì ì¸ ì›¹ í¬ë¡¤ë§ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ë´‡ ê°ì§€ ìš°íšŒì™€ ê´€ë ¨ëœ ê¸°ìˆ ë“¤ì„ ì ì ˆíˆ í™œìš©í•˜ë©´ì„œë„, ì›¹ì‚¬ì´íŠ¸ì˜ ì •ì±…ì„ ì¡´ì¤‘í•˜ëŠ” ê· í˜• ì¡íŒ ì ‘ê·¼ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ìš°íšŒë¥¼ ìœ„í•œ ëœë¤ ë™ì‘
            simulator.random_mouse_movement()
            simulator.random_scroll()
            
            # í˜ì´ì§€ í¬ë¡¤ë§
            content = crawler.get_page(url)
            
            # ë°ì´í„° ì²˜ë¦¬ ë° ì €ì¥
            cleaned_data = DataCleaner.clean_text(content)
            data_handler.save_item({
                'url': url,
                'content': cleaned_data,
                'timestamp': datetime.now()
            })
            
    finally:
        crawler.driver.quit()
```

### ì£¼ì˜ì‚¬í•­

1. **ì›¹ì‚¬ì´íŠ¸ ì •ì±… ì¤€ìˆ˜**
   - robots.txt ê·œì¹™ í™•ì¸
   - í¬ë¡¤ë§ ê°„ê²© ì¡°ì ˆ
   - ì„œë²„ ë¶€í•˜ ê³ ë ¤

2. **ì—ëŸ¬ ì²˜ë¦¬**
   - ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ëŒ€ë¹„
   - íƒ€ì„ì•„ì›ƒ ì„¤ì •
   - ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„
   - ì˜ˆì™¸ ìƒí™© ë¡œê¹…

3. **ë³´ì•ˆ ê³ ë ¤ì‚¬í•­**
   - ë¯¼ê°í•œ ì •ë³´ ì•”í˜¸í™”
   - ì ‘ê·¼ ê¶Œí•œ ê´€ë¦¬
   - IP ì°¨ë‹¨ ë°©ì§€

## 8. ê³ ê¸‰ ë³´ì•ˆ ë° ìš°íšŒ ê¸°ë²•

### 8.1 ë¸Œë¼ìš°ì € í”„ë¡œí•„ ê´€ë¦¬

```python
class BrowserProfileManager:
    def __init__(self):
        self.profiles_dir = "browser_profiles"
        os.makedirs(self.profiles_dir, exist_ok=True)
        
    def create_profile(self, profile_name):
        """ìƒˆë¡œìš´ ë¸Œë¼ìš°ì € í”„ë¡œí•„ ìƒì„±"""
        profile_path = os.path.join(self.profiles_dir, profile_name)
        
        options = Options()
        options.add_argument(f'--user-data-dir={profile_path}')
        options.add_argument('--profile-directory=Default')
        
        # ê¸°ë³¸ ì„¤ì • ì¶”ê°€
        prefs = {
            'profile.default_content_setting_values.notifications': 2,
            'profile.password_manager_enabled': False,
            'profile.managed_default_content_settings.images': 2
        }
        options.add_experimental_option('prefs', prefs)
        
        return options
        
    def rotate_profile(self):
        """í”„ë¡œí•„ ë¡œí…Œì´ì…˜"""
        profiles = os.listdir(self.profiles_dir)
        return random.choice(profiles) if profiles else 'default'
```

### 8.2 ê³ ê¸‰ ë´‡ ê°ì§€ ìš°íšŒ

```python
class AdvancedAntiDetection:
    def __init__(self, driver):
        self.driver = driver
        
    def bypass_selenium_detection(self):
        """Selenium ê°ì§€ ìš°íšŒ"""
        # navigator.webdriver ì†ì„± ì œê±°
        self.driver.execute_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
        """)
        
        # ìë™í™” ê´€ë ¨ ì†ì„± ì œê±°
        self.driver.execute_script("""
            delete window.cdc_adoQpoasnfa76pfcZLmcfl_Array;
            delete window.cdc_adoQpoasnfa76pfcZLmcfl_Promise;
            delete window.cdc_adoQpoasnfa76pfcZLmcfl_Symbol;
        """)
        
    def emulate_human_timing(self):
        """ì¸ê°„ë‹¤ìš´ íƒ€ì´ë° íŒ¨í„´ ìƒì„±"""
        def random_delay():
            # ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ì‚¬ìš©í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì§€ì—° ì‹œê°„
            mean_delay = 2
            std_dev = 0.5
            delay = random.gauss(mean_delay, std_dev)
            return max(0.1, delay)
            
        return random_delay()
        
    def randomize_mouse_movements(self):
        """ìì—°ìŠ¤ëŸ¬ìš´ ë§ˆìš°ìŠ¤ ì›€ì§ì„ ìƒì„±"""
        def bezier_curve(p0, p1, p2, t):
            # ë² ì§€ì–´ ê³¡ì„ ì„ ì‚¬ìš©í•œ ìì—°ìŠ¤ëŸ¬ìš´ ë§ˆìš°ìŠ¤ ì›€ì§ì„
            x = (1-t)**2 * p0[0] + 2*(1-t)*t * p1[0] + t**2 * p2[0]
            y = (1-t)**2 * p0[1] + 2*(1-t)*t * p1[1] + t**2 * p2[1]
            return (x, y)
            
        # ì‹œì‘ì ê³¼ ëì  ì„¤ì •
        start = (0, 0)
        end = (random.randint(100, 500), random.randint(100, 500))
        
        # ì œì–´ì  ì„¤ì •
        control = (random.randint(min(start[0], end[0]), max(start[0], end[0])),
                  random.randint(min(start[1], end[1]), max(start[1], end[1])))
                  
        # ê³¡ì„ ì„ ë”°ë¼ ì›€ì§ì„
        points = [bezier_curve(start, control, end, t/100) for t in range(101)]
        
        actions = ActionChains(self.driver)
        for point in points:
            actions.move_by_offset(*point)
            actions.pause(random.uniform(0.001, 0.003))
        
        actions.perform()
```

### 8.3 ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ ì œì–´

```python
class NetworkTrafficController:
    def __init__(self, driver):
        self.driver = driver
        
    def set_request_interceptor(self):
        """ìš”ì²­ ì¸í„°ì…‰í„° ì„¤ì •"""
        def interceptor(request):
            # ë¶ˆí•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨
            if request.resource_type in ['image', 'media', 'font']:
                request.abort()
            else:
                request.continue_()
                
        self.driver.request_interceptor = interceptor
        
    def modify_headers(self):
        """í—¤ë” ìˆ˜ì •"""
        def header_modifier(request):
            # ê¸°ì¡´ í—¤ë” ìˆ˜ì •
            headers = request.headers
            headers['Accept-Language'] = random.choice([
                'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
                'en-US,en;q=0.9,ko;q=0.8',
                'ja-JP,ja;q=0.9,en-US;q=0.8,en;q=0.7'
            ])
            request.headers = headers
            
        self.driver.header_modifier = header_modifier
```

### 8.4 ê³ ê¸‰ ìº¡ì°¨ ì²˜ë¦¬

```python
class AdvancedCaptchaSolver:
    def __init__(self, api_key=None):
        self.api_key = api_key
        self.audio_to_text = None  # ìŒì„±ì¸ì‹ ì—”ì§„ ì´ˆê¸°í™”
        
    async def solve_audio_captcha(self, audio_url):
        """ìŒì„± ìº¡ì°¨ í•´ê²°"""
        try:
            # ìŒì„± íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            async with aiohttp.ClientSession() as session:
                async with session.get(audio_url) as response:
                    audio_content = await response.read()
                    
            # ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            text = self.audio_to_text.recognize(audio_content)
            return text
            
        except Exception as e:
            print(f"ìŒì„± ìº¡ì°¨ í•´ê²° ì‹¤íŒ¨: {str(e)}")
            return None
            
    def solve_puzzle_captcha(self, image_path):
        """í¼ì¦ ìº¡ì°¨ í•´ê²°"""
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            image = cv2.imread(image_path)
            
            # í¼ì¦ ì¡°ê° ì°¾ê¸°
            edges = cv2.Canny(image, 100, 200)
            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            # í¼ì¦ ì¡°ê° ë§¤ì¹­
            # ... í¼ì¦ ë§¤ì¹­ ë¡œì§ ...
            
            return {'x': x_offset, 'y': y_offset}
            
        except Exception as e:
            print(f"í¼ì¦ ìº¡ì°¨ í•´ê²° ì‹¤íŒ¨: {str(e)}")
            return None
```

## 9. ê³ ê¸‰ ë°ì´í„° ì²˜ë¦¬

### 9.1 ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”

```python
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor

class ParallelProcessor:
    def __init__(self, max_workers=None):
        self.thread_executor = ThreadPoolExecutor(max_workers=max_workers)
        self.process_executor = ProcessPoolExecutor(max_workers=max_workers)
        
    def process_data_parallel(self, items, processor_func):
        """ë°ì´í„° ë³‘ë ¬ ì²˜ë¦¬"""
        # CPU ë°”ìš´ë“œ ì‘ì—…ì€ ProcessPoolExecutor ì‚¬ìš©
        if self._is_cpu_bound(processor_func):
            executor = self.process_executor
        else:
            # I/O ë°”ìš´ë“œ ì‘ì—…ì€ ThreadPoolExecutor ì‚¬ìš©
            executor = self.thread_executor
            
        results = list(executor.map(processor_func, items))
        return results
        
    @staticmethod
    def _is_cpu_bound(func):
        """ì‘ì—…ì´ CPU ë°”ìš´ë“œì¸ì§€ íŒë‹¨"""
        # í•¨ìˆ˜ì˜ íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬ CPU/IO ë°”ìš´ë“œ íŒë‹¨
        # ... íŒë‹¨ ë¡œì§ ...
        return True  # ì˜ˆì‹œ
```

### 9.2 ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì²˜ë¦¬

```python
class StreamProcessor:
    def __init__(self, chunk_size=1000):
        self.chunk_size = chunk_size
        self.buffer = []
        
    async def process_stream(self, data_stream, processor_func):
        """ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì²˜ë¦¬"""
        async for item in data_stream:
            self.buffer.append(item)
            
            if len(self.buffer) >= self.chunk_size:
                await self._process_buffer(processor_func)
                
        # ë‚¨ì€ ë°ì´í„° ì²˜ë¦¬
        if self.buffer:
            await self._process_buffer(processor_func)
            
    async def _process_buffer(self, processor_func):
        """ë²„í¼ ë°ì´í„° ì²˜ë¦¬"""
        try:
            results = await processor_func(self.buffer)
            self.buffer = []
            return results
        except Exception as e:
            print(f"ë²„í¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return None
```

### 9.3 ë°ì´í„° ê²€ì¦ ë° ì •ì œ

```python
class DataValidator:
    def __init__(self):
        self.schema = None
        self.rules = []
        
    def add_validation_rule(self, field, rule_func):
        """ê²€ì¦ ê·œì¹™ ì¶”ê°€"""
        self.rules.append((field, rule_func))
        
    def validate_data(self, data):
        """ë°ì´í„° ê²€ì¦"""
        errors = []
        
        for field, rule_func in self.rules:
            if field in data:
                try:
                    if not rule_func(data[field]):
                        errors.append(f"Validation failed for {field}")
                except Exception as e:
                    errors.append(f"Error validating {field}: {str(e)}")
                    
        return len(errors) == 0, errors

class DataCleaner:
    @staticmethod
    def clean_html(text):
        """HTML íƒœê·¸ ì œê±°"""
        soup = BeautifulSoup(text, 'html.parser')
        return soup.get_text()
        
    @staticmethod
    def normalize_whitespace(text):
        """ê³µë°± ì •ê·œí™”"""
        return ' '.join(text.split())
        
    @staticmethod
    def remove_special_chars(text, keep_chars=''):
        """íŠ¹ìˆ˜ë¬¸ì ì œê±°"""
        pattern = f'[^\\w\\s{re.escape(keep_chars)}]'
        return re.sub(pattern, '', text)
```

## 10. ë°°í¬ ë° ëª¨ë‹ˆí„°ë§

### 10.1 ë¡œê¹… ì‹œìŠ¤í…œ

```python
import logging
from logging.handlers import RotatingFileHandler

class CrawlerLogger:
    def __init__(self, name):
        self.logger = logging.getLogger(name)
        self.setup_logger()
        
    def setup_logger(self):
        """ë¡œê±° ì„¤ì •"""
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
        # íŒŒì¼ í•¸ë“¤ëŸ¬ ì„¤ì •
        file_handler = RotatingFileHandler(
            'crawler.log',
            maxBytes=10000000,  # 10MB
            backupCount=5
        )
        file_handler.setFormatter(formatter)
        
        # ìŠ¤íŠ¸ë¦¼ í•¸ë“¤ëŸ¬ ì„¤ì •
        stream_handler = logging.StreamHandler()
        stream_handler.setFormatter(formatter)
        
        self.logger.addHandler(file_handler)
        self.logger.addHandler(stream_handler)
        self.logger.setLevel(logging.INFO)
```

### 10.2 ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```python
class PerformanceMonitor:
    def __init__(self):
        self.metrics = {}
        
    def start_monitoring(self, task_name):
        """ì‘ì—… ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.metrics[task_name] = {
            'start_time': time.time(),
            'success_count': 0,
            'error_count': 0,
            'response_times': []
        }
        
    def record_metric(self, task_name, success=True, response_time=None):
        """ë©”íŠ¸ë¦­ ê¸°ë¡"""
        if task_name in self.metrics:
            if success:
                self.metrics[task_name]['success_count'] += 1
            else:
                self.metrics[task_name]['error_count'] += 1
                
            if response_time:
                self.metrics[task_name]['response_times'].append(response_time)
                
    def get_statistics(self, task_name):
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        if task_name in self.metrics:
            stats = self.metrics[task_name]
            total_time = time.time() - stats['start_time']
            
            return {
                'total_time': total_time,
                'success_rate': stats['success_count'] / (stats['success_count'] + stats['error_count']),
                'average_response_time': sum(stats['response_times']) / len(stats['response_times']) if stats['response_times'] else 0,
                'requests_per_second': (stats['success_count'] + stats['error_count']) / total_time
            }
        return None
```

## ì‹¤ì œ ì‚¬ìš© ì˜ˆì œ

### ì „ì²´ ì‹œìŠ¤í…œ í†µí•© ì˜ˆì œ

"""
í†µí•© í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì™„ì„±ë³¸
- ë¹„ë™ê¸° ì²˜ë¦¬
- ìº¡ì°¨ ì²˜ë¦¬
- ë´‡ ê°ì§€ ìš°íšŒ
- ë°ì´í„° ì²˜ë¦¬
- ì˜¤ë¥˜ ì²˜ë¦¬
- ëª¨ë‹ˆí„°ë§
"""

'''python
import asyncio
import time
import random
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from selenium.webdriver import ActionChains

class IntegratedCrawlingSystem:
    def __init__(self):
        self.browser_manager = BrowserProfileManager()
        self.anti_detection = None
        self.logger = CrawlerLogger('IntegratedCrawler')
        self.monitor = PerformanceMonitor()
        self.data_handler = DatabaseHandler()
        self.captcha_solver = AdvancedCaptchaSolver()
        self.proxy_manager = ProxyManager()
        
    async def initialize_system(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            # í”„ë¡ì‹œ í’€ ì´ˆê¸°í™”
            await self.proxy_manager.initialize_proxy_pool()
            
            # ë¸Œë¼ìš°ì € í”„ë¡œí•„ ì¤€ë¹„
            await self.browser_manager.prepare_profiles()
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸
            await self.data_handler.verify_connection()
            
            self.logger.logger.info("System initialized successfully")
            return True
            
        except Exception as e:
            self.logger.logger.error(f"System initialization failed: {str(e)}")
            return False
            
    async def run_crawling_task(self, urls, retry_count=3):
        """í¬ë¡¤ë§ ì‘ì—… ì‹¤í–‰"""
        self.monitor.start_monitoring('crawling_task')
        results = []
        driver = None
        
        try:
            # ë¸Œë¼ìš°ì € ì„¤ì •
            options = await self.browser_manager.create_profile('main')
            driver = await self._setup_driver(options)
            
            # ê° URL ì²˜ë¦¬
            for url in urls:
                try:
                    self.logger.logger.info(f"Processing URL: {url}")
                    result = await self._process_single_url(driver, url, retry_count)
                    if result:
                        results.append(result)
                        
                except Exception as e:
                    self.logger.logger.error(f"Error processing URL {url}: {str(e)}")
                    continue
                    
            return results
            
        except Exception as e:
            self.logger.logger.error(f"Critical error in crawling task: {str(e)}")
            raise
            
        finally:
            if driver:
                await self._cleanup(driver)
                
    async def _setup_driver(self, options):
        """ì›¹ë“œë¼ì´ë²„ ì„¤ì •"""
        driver = webdriver.Chrome(options=options)
        
        # ë´‡ ê°ì§€ ìš°íšŒ ì„¤ì •
        self.anti_detection = AdvancedAntiDetection(driver)
        await self.anti_detection.setup_advanced_evasion()
        
        # ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ ì œì–´
        network_controller = NetworkTrafficController(driver)
        await network_controller.setup_traffic_control()
        
        return driver
        
    async def _process_single_url(self, driver, url, retry_count):
        """ë‹¨ì¼ URL ì²˜ë¦¬"""
        for attempt in range(retry_count):
            try:
                # í”„ë¡ì‹œ ë¡œí…Œì´ì…˜ (ì²« ì‹œë„ ì œì™¸)
                if attempt > 0:
                    await self.proxy_manager.rotate_proxy(driver)
                    
                # ì¸ê°„ë‹¤ìš´ í–‰ë™ ì‹œë®¬ë ˆì´ì…˜
                await self._simulate_human_behavior(driver)
                
                # í˜ì´ì§€ í¬ë¡¤ë§
                start_time = time.time()
                content = await self._crawl_page(driver, url)
                response_time = time.time() - start_time
                
                if content:
                    # ìº¡ì°¨ í™•ì¸ ë° ì²˜ë¦¬
                    if await self._check_for_captcha(driver):
                        await self._handle_captcha(driver)
                    
                    # ë°ì´í„° ì²˜ë¦¬
                    processed_data = await self._process_data(content, url)
                    
                    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡
                    self.monitor.record_metric('crawling_task', True, response_time)
                    
                    return processed_data
                    
            except CaptchaException as ce:
                self.logger.logger.warning(f"Captcha detected on {url}, attempt {attempt + 1}")
                if attempt == retry_count - 1:
                    raise ce
                continue
                
            except Exception as e:
                self.logger.logger.error(f"Error on attempt {attempt + 1} for {url}: {str(e)}")
                if attempt == retry_count - 1:
                    self.monitor.record_metric('crawling_task', False)
                    raise e
                
                # ì§€ìˆ˜ ë°±ì˜¤í”„
                await asyncio.sleep(2 ** attempt)
                
        return None
        
    async def _simulate_human_behavior(self, driver):
        """ì¸ê°„ë‹¤ìš´ í–‰ë™ ì‹œë®¬ë ˆì´ì…˜"""
        try:
            # ëœë¤ ìŠ¤í¬ë¡¤
            scroll_amount = random.randint(100, 500)
            driver.execute_script(f"window.scrollBy(0, {scroll_amount});")
            
            # ëœë¤ ë§ˆìš°ìŠ¤ ì›€ì§ì„
            actions = ActionChains(driver)
            x_offset = random.randint(-100, 100)
            y_offset = random.randint(-100, 100)
            actions.move_by_offset(x_offset, y_offset)
            actions.perform()
            
            # ëœë¤ ëŒ€ê¸°
            await asyncio.sleep(random.uniform(1, 3))
            
        except Exception as e:
            self.logger.logger.error(f"Error in human behavior simulation: {str(e)}")
            
    async def _crawl_page(self, driver, url):
        """í˜ì´ì§€ í¬ë¡¤ë§"""
        try:
            # í˜ì´ì§€ ë¡œë“œ
            driver.get(url)
            
            # ë™ì  ì½˜í…ì¸  ëŒ€ê¸°
            dynamic_handler = DynamicContentHandler(driver)
            await dynamic_handler.wait_for_dynamic_content()
            
            # ë¬´í•œ ìŠ¤í¬ë¡¤ ì²˜ë¦¬
            if await self._has_infinite_scroll(driver):
                await dynamic_handler.handle_infinite_scroll()
            
            # AJAX ìš”ì²­ ì™„ë£Œ ëŒ€ê¸°
            ajax_handler = AjaxRequestHandler(driver)
            await ajax_handler.wait_for_ajax_completion()
            
            return driver.page_source
            
        except TimeoutException:
            self.logger.logger.error(f"Timeout while loading {url}")
            return None
            
        except Exception as e:
            self.logger.logger.error(f"Error in crawl_page: {str(e)}")
            return None
            
    async def _process_data(self, content, url):
        """ë°ì´í„° ì²˜ë¦¬"""
        try:
            # ë°ì´í„° ì •ì œ
            cleaned_data = await self._clean_data(content)
            
            # ë°ì´í„° ê²€ì¦
            if not await self._validate_data(cleaned_data):
                return None
            
            # ë°ì´í„° ë³€í™˜
            transformed_data = await self._transform_data(cleaned_data, url)
            
            # ë°ì´í„° ì €ì¥
            if await self.data_handler.store_data(transformed_data):
                self.logger.logger.info(f"Data successfully stored for {url}")
                return transformed_data
                
            return None
            
        except Exception as e:
            self.logger.logger.error(f"Error processing data: {str(e)}")
            return None
            
    async def _clean_data(self, content):
        """ë°ì´í„° ì •ì œ"""
        cleaner = DataCleaner()
        return await cleaner.clean_content(content)
        
    async def _validate_data(self, data):
        """ë°ì´í„° ê²€ì¦"""
        validator = DataValidator()
        is_valid, errors = await validator.validate_data(data)
        
        if not is_valid:
            self.logger.logger.warning(f"Data validation errors: {errors}")
            
        return is_valid
        
    async def _transform_data(self, data, url):
        """ë°ì´í„° ë³€í™˜"""
        return {
            'url': url,
            'content': data,
            'timestamp': datetime.now(),
            'metadata': {
                'crawler_version': '1.0',
                'processing_time': time.time()
            }
        }
        
    async def _check_for_captcha(self, driver):
        """ìº¡ì°¨ í™•ì¸"""
        captcha_indicators = [
            '//iframe[contains(@src, "recaptcha")]',
            '//iframe[contains(@src, "hcaptcha")]',
            '//*[contains(text(), "captcha")]'
        ]
        
        for indicator in captcha_indicators:
            try:
                element = driver.find_element(By.XPATH, indicator)
                if element.is_displayed():
                    return True
            except NoSuchElementException:
                continue
                
        return False
        
    async def _handle_captcha(self, driver):
        """ìº¡ì°¨ ì²˜ë¦¬"""
        try:
            captcha_type = await self._detect_captcha_type(driver)
            
            if captcha_type == 'recaptcha':
                solved = await self.captcha_solver.solve_recaptcha(driver)
            elif captcha_type == 'hcaptcha':
                solved = await self.captcha_solver.solve_hcaptcha(driver)
            else:
                solved = await self.captcha_solver.solve_image_captcha(driver)
                
            if not solved:
                raise CaptchaException("Failed to solve captcha")
                
        except Exception as e:
            raise CaptchaException(f"Error handling captcha: {str(e)}")
            
    async def _detect_captcha_type(self, driver):
        """ìº¡ì°¨ ìœ í˜• ê°ì§€"""
        if driver.find_elements(By.XPATH, '//iframe[contains(@src, "recaptcha")]'):
            return 'recaptcha'
        elif driver.find_elements(By.XPATH, '//iframe[contains(@src, "hcaptcha")]'):
            return 'hcaptcha'
        else:
            return 'image'
            
    async def _has_infinite_scroll(self, driver):
        """ë¬´í•œ ìŠ¤í¬ë¡¤ í™•ì¸"""
        try:
            last_height = driver.execute_script("return document.body.scrollHeight")
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            await asyncio.sleep(2)
            new_height = driver.execute_script("return document.body.scrollHeight")
            
            return new_height > last_height
            
        except Exception:
            return False
            
    async def _cleanup(self, driver):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if driver:
                driver.quit()
            
            await self.browser_manager.cleanup_profiles()
            await self.data_handler.close_connection()
            
            self.logger.logger.info("Cleanup completed successfully")
            
        except Exception as e:
            self.logger.logger.error(f"Error during cleanup: {str(e)}")


# ì‚¬ìš© ì˜ˆì œ
async def main():
    # í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    crawler = IntegratedCrawlingSystem()
    
    # í¬ë¡¤ë§í•  URL ëª©ë¡
    urls = [
        "https://example.com/page1",
        "https://example.com/page2",
        "https://example.com/page3"
    ]
    
    try:
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if not await crawler.initialize_system():
            raise Exception("System initialization failed")
            
        # í¬ë¡¤ë§ ì‹¤í–‰
        results = await crawler.run_crawling_task(urls)
        
        # ê²°ê³¼ ì²˜ë¦¬
        if results:
            print(f"Successfully crawled {len(results)} pages")
            for result in results:
                print(f"Data from {result['url']}:")
                print(f"- Timestamp: {result['timestamp']}")
                print(f"- Content length: {len(result['content'])}")
                print("---")
                
    except Exception as e:
        print(f"Error in main: {str(e)}")
        
    finally:
        # ì‹œìŠ¤í…œ ì •ë¦¬
        await crawler._cleanup(None)

# ì‹¤í–‰
if __name__ == "__main__":
    asyncio.run(main())
'''