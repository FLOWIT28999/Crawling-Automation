# 웹 크롤링 & 자동화 완벽 가이드

> 이 가이드는 Python을 사용한 웹 크롤링과 자동화에 대한 포괄적인 안내를 제공합니다.

## 📚 목차

1. [개요 및 준비](#1-개요-및-준비)
2. [기본 크롤링 설정](#2-기본-크롤링-설정)
3. [봇 감지 우회 기법](#3-봇-감지-우회-기법)
4. [고급 크롤링 기술](#4-고급-크롤링-기술)
5. [데이터 처리 및 저장](#5-데이터-처리-및-저장)
6. [성능 최적화](#6-성능-최적화)
7. [모범 사례 및 주의사항](#7-모범-사례-및-주의사항)

## 1. 개요 및 준비

### 1.1 필요한 라이브러리 설치

```bash
# 기본 크롤링 도구
pip install requests beautifulsoup4
pip install selenium webdriver_manager

# 봇 감지 우회 도구
pip install fake-useragent
pip install undetected-chromedriver
pip install python-anticaptcha

# 성능 최적화 도구
pip install aiohttp
pip install redis celery
```

### 1.2 기본 설정

```python
import random
import time
from fake_useragent import UserAgent
import undetected_chromedriver as uc
from selenium.webdriver.chrome.options import Options

def setup_base_configuration():
    ua = UserAgent()
    
    chrome_options = Options()
    chrome_options.add_argument(f'user-agent={ua.random}')
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    
    return chrome_options
```

## 2. 기본 크롤링 설정

### 2.1 헤더 및 세션 관리

```python
def create_headers():
    ua = UserAgent()
    return {
        'User-Agent': ua.random,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'DNT': '1',
        'Upgrade-Insecure-Requests': '1'
    }

class SessionManager:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers = create_headers()
        self.session.cookies.clear()

    def rotate_session(self):
        self.session.close()
        self.session = requests.Session()
        self.session.headers = create_headers()
```

### 2.2 기본 크롤러 클래스

```python
class BaseCrawler:
    def __init__(self):
        self.session_manager = SessionManager()
        self.driver = self._setup_driver()

    def _setup_driver(self):
        options = setup_base_configuration()
        return uc.Chrome(options=options)

    def get_page(self, url, use_selenium=False):
        if use_selenium:
            self.driver.get(url)
            return self.driver.page_source
        else:
            response = self.session_manager.session.get(url)
            return response.text
```

## 3. 봇 감지 우회 기법

### 3.1 브라우저 지문 무작위화

```python
class BrowserFingerprint:
    @staticmethod
    def apply_random_fingerprint(driver):
        # WebGL 렌더러 정보 수정
        driver.execute_script("""
            const getParameter = WebGLRenderingContext.prototype.getParameter;
            WebGLRenderingContext.prototype.getParameter = function(parameter) {
                if (parameter === 37445) {
                    return 'Intel Open Source Technology Center';
                }
                if (parameter === 37446) {
                    return 'Mesa DRI Intel(R) Sandybridge Mobile';
                }
                return getParameter.apply(this, arguments);
            };
        """)

        # 화면 해상도 무작위화
        driver.execute_script(f"""
            Object.defineProperty(window.screen, 'width', {{
                get: function() {{ return {random.choice([1366, 1440, 1920])}; }}
            }});
            Object.defineProperty(window.screen, 'height', {{
                get: function() {{ return {random.choice([768, 900, 1080])}; }}
            }});
        """)
```

### 3.2 인간다운 행동 시뮬레이션

```python
class HumanBehaviorSimulator:
    def __init__(self, driver):
        self.driver = driver
        
    def random_scroll(self):
        """자연스러운 스크롤 동작 수행"""
        scroll_amount = random.randint(100, 500)
        self.driver.execute_script(f"window.scrollBy(0, {scroll_amount});")
        time.sleep(random.uniform(0.5, 1.5))
        
    def random_mouse_movement(self):
        """자연스러운 마우스 움직임 시뮬레이션"""
        actions = ActionChains(self.driver)
        x_offset = random.randint(-100, 100)
        y_offset = random.randint(-100, 100)
        actions.move_by_offset(x_offset, y_offset)
        actions.perform()
        time.sleep(random.uniform(0.1, 0.3))
        
    def type_like_human(self, element, text):
        """인간다운 타이핑 시뮬레이션"""
        for char in text:
            element.send_keys(char)
            time.sleep(random.uniform(0.1, 0.3))
```

### 3.3 네트워크 패턴 다양화

```python
class NetworkPatternRandomizer:
    def __init__(self):
        self.proxy_pool = self._load_proxy_pool()
        
    def _load_proxy_pool(self):
        # 프록시 목록 로드 (예시)
        return [
            'http://proxy1.example.com:8080',
            'http://proxy2.example.com:8080',
            'http://proxy3.example.com:8080'
        ]
        
    def get_random_proxy(self):
        return random.choice(self.proxy_pool)
        
    def random_delay(self):
        time.sleep(random.uniform(2, 5))
```

## 4. 고급 크롤링 기술

### 4.1 동적 콘텐츠 처리

```python
class DynamicContentHandler:
    def __init__(self, driver):
        self.driver = driver
        
    def handle_infinite_scroll(self):
        """무한 스크롤 처리"""
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        
        while True:
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(random.uniform(1, 2))
            
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
                
            last_height = new_height
            
    def wait_for_dynamic_content(self, selector, timeout=10):
        """동적으로 로드되는 컨텐츠 대기"""
        try:
            element = WebDriverWait(self.driver, timeout).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, selector))
            )
            return element
        except TimeoutException:
            return None
```

### 4.2 캡차(CAPTCHA) 처리

```python
class CaptchaHandler:
    def __init__(self, api_key):
        self.solver = AnticaptchaClient(api_key)
        
    def solve_recaptcha(self, site_key, url):
        task = NoCaptchaTaskProxyless(url, site_key)
        job = self.solver.createTask(task)
        job.join()
        return job.get_solution_response()
        
    def solve_image_captcha(self, image_path):
        with open(image_path, 'rb') as image_file:
            task = ImageToTextTask(image_file)
            job = self.solver.createTask(task)
            job.join()
            return job.get_captcha_text()
```

## 5. 데이터 처리 및 저장

### 5.1 데이터 정제

```python
class DataCleaner:
    @staticmethod
    def clean_text(text):
        """텍스트 데이터 정제"""
        # 불필요한 공백 제거
        text = ' '.join(text.split())
        
        # HTML 태그 제거
        text = re.sub(r'<[^>]+>', '', text)
        
        # 특수문자 처리
        text = re.sub(r'[^\w\s]', ' ', text)
        
        return text.strip()
        
    @staticmethod
    def validate_data(item):
        """데이터 유효성 검사"""
        required_fields = ['title', 'content', 'date']
        return all(field in item and item[field] for field in required_fields)
```

### 5.2 데이터베이스 저장

```python
class DatabaseHandler:
    def __init__(self):
        self.client = MongoClient('mongodb://localhost:27017/')
        self.db = self.client['crawler_db']
        
    def save_item(self, item):
        """데이터 저장"""
        if not DataCleaner.validate_data(item):
            return False
            
        collection = self.db['crawled_data']
        try:
            collection.insert_one(item)
            return True
        except Exception as e:
            print(f"저장 실패: {str(e)}")
            return False
```

## 6. 성능 최적화

### 6.1 비동기 크롤링

```python
class AsyncCrawler:
    def __init__(self, max_concurrency=5):
        self.semaphore = asyncio.Semaphore(max_concurrency)
        
    async def crawl_url(self, url, session):
        async with self.semaphore:
            try:
                async with session.get(url) as response:
                    return await response.text()
            except Exception as e:
                print(f"Error crawling {url}: {str(e)}")
                return None
                
    async def crawl_multiple_urls(self, urls):
        async with aiohttp.ClientSession() as session:
            tasks = [self.crawl_url(url, session) for url in urls]
            return await asyncio.gather(*tasks)
```

### 6.2 분산 처리

```python
class DistributedCrawler:
    def __init__(self):
        self.celery = Celery('crawler', broker='redis://localhost:6379/0')
        
    @self.celery.task
    def crawl_task(self, url):
        crawler = BaseCrawler()
        return crawler.get_page(url)
        
    def start_distributed_crawling(self, urls):
        return [self.crawl_task.delay(url) for url in urls]
```

## 7. 모범 사례 및 주의사항

### 7.1 리소스 관리

```python
class ResourceManager:
    def __init__(self):
        self.active_connections = []
        
    def add_connection(self, connection):
        self.active_connections.append(connection)
        
    def cleanup(self):
        for connection in self.active_connections:
            try:
                connection.close()
            except Exception as e:
                print(f"리소스 정리 실패: {str(e)}")
```

### 7.2 오류 처리 및 복구

```python
class ErrorHandler:
    @staticmethod
    def handle_connection_error(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except requests.exceptions.RequestException as e:
                    print(f"연결 오류 (시도 {attempt + 1}/{max_retries}): {str(e)}")
                    if attempt < max_retries - 1:
                        time.sleep(random.uniform(1, 3))
                    else:
                        raise
        return wrapper
```

### 실행 예제

```python
if __name__ == "__main__":
    # 크롤러 초기화
    crawler = BaseCrawler()
    simulator = HumanBehaviorSimulator(crawler.driver)
    data_handler = DatabaseHandler()
    
    try:
        # 크롤링 수행
        urls = ["http://example.com/page1", "http://example.com/page2"]
        for url in urls:
            # 봇 감지 우회 설정
            self.anti_detection = AdvancedAntiDetection(driver)
            self.anti_detection.bypass_selenium_detection()
            
            # 네트워크 트래픽 제어
            network_controller = NetworkTrafficController(driver)
            network_controller.set_request_interceptor()
            network_controller.modify_headers()
            
            results = []
            for url in urls:
                try:
                    # 인간다운 행동 시뮬레이션
                    self.anti_detection.randomize_mouse_movements()
                    await asyncio.sleep(self.anti_detection.emulate_human_timing())
                    
                    # 페이지 크롤링
                    start_time = time.time()
                    content = await self.crawl_page(driver, url)
                    response_time = time.time() - start_time
                    
                    if content:
                        # 데이터 처리
                        cleaned_data = self.process_data(content)
                        results.append(cleaned_data)
                        
                        # 성능 메트릭 기록
                        self.monitor.record_metric('crawling_task', success=True, response_time=response_time)
                    else:
                        self.monitor.record_metric('crawling_task', success=False)
                        
                except Exception as e:
                    self.logger.logger.error(f"Error crawling {url}: {str(e)}")
                    self.monitor.record_metric('crawling_task', success=False)
                    
            return results
            
        finally:
            driver.quit()
            
    async def crawl_page(self, driver, url):
        """단일 페이지 크롤링"""
        try:
            driver.get(url)
            await asyncio.sleep(2)  # 페이지 로딩 대기
            
            # 동적 컨텐츠 처리
            dynamic_handler = DynamicContentHandler(driver)
            dynamic_handler.handle_infinite_scroll()
            
            return driver.page_source
            
        except Exception as e:
            self.logger.logger.error(f"Error in crawl_page: {str(e)}")
            return None
            
    def process_data(self, content):
        """데이터 처리 및 정제"""
        cleaner = DataCleaner()
        validator = DataValidator()
        
        # 데이터 정제
        cleaned_content = cleaner.clean_html(content)
        cleaned_content = cleaner.normalize_whitespace(cleaned_content)
        cleaned_content = cleaner.remove_special_chars(cleaned_content)
        
        # 데이터 검증
        data = {'content': cleaned_content}
        is_valid, errors = validator.validate_data(data)
        
        if not is_valid:
            self.logger.logger.warning(f"Data validation errors: {errors}")
            
        return data
```

## 11. 실전 응용 시나리오

### 11.1 대규모 데이터 수집 시스템

```python
class LargeScaleCrawler:
    def __init__(self):
        self.distributed_crawler = DistributedCrawler()
        self.stream_processor = StreamProcessor()
        self.parallel_processor = ParallelProcessor()
        
    async def run_large_scale_crawl(self, url_list):
        """대규모 크롤링 작업 실행"""
        # URL 청크로 분할
        url_chunks = [url_list[i:i+1000] for i in range(0, len(url_list), 1000)]
        
        results = []
        for chunk in url_chunks:
            # 분산 크롤링 작업 시작
            crawl_tasks = self.distributed_crawler.start_distributed_crawling(chunk)
            
            # 스트리밍 방식으로 데이터 처리
            async for data in self.stream_processor.process_stream(crawl_tasks, self.process_data):
                if data:
                    results.extend(data)
                    
        return results
        
    def process_data(self, data_chunk):
        """병렬 데이터 처리"""
        return self.parallel_processor.process_data_parallel(
            data_chunk,
            self._process_single_item
        )
        
    def _process_single_item(self, item):
        """단일 데이터 아이템 처리"""
        try:
            # 데이터 정제 및 검증
            cleaner = DataCleaner()
            cleaned_item = {
                'content': cleaner.clean_html(item.get('content', '')),
                'timestamp': datetime.now()
            }
            
            return cleaned_item
            
        except Exception as e:
            print(f"데이터 처리 실패: {str(e)}")
            return None
```

### 11.2 지능형 크롤링 시스템

```python
class IntelligentCrawler:
    def __init__(self):
        self.url_patterns = {}
        self.content_analyzer = None
        
    def learn_patterns(self, training_data):
        """패턴 학습"""
        for url, content in training_data:
            # URL 패턴 분석
            pattern = self._extract_url_pattern(url)
            
            # 콘텐츠 패턴 분석
            content_pattern = self._analyze_content_structure(content)
            
            self.url_patterns[pattern] = content_pattern
            
    def _extract_url_pattern(self, url):
        """URL 패턴 추출"""
        # URL 구조 분석
        parsed_url = urlparse(url)
        path_parts = parsed_url.path.split('/')
        
        # 패턴 생성
        pattern_parts = []
        for part in path_parts:
            if part.isdigit():
                pattern_parts.append('{\d+}')
            else:
                pattern_parts.append(part)
                
        return '/'.join(pattern_parts)
        
    def _analyze_content_structure(self, content):
        """콘텐츠 구조 분석"""
        soup = BeautifulSoup(content, 'html.parser')
        structure = {}
        
        # 주요 콘텐츠 영역 식별
        for element in soup.find_all(['div', 'article', 'section']):
            if element.get('class'):
                structure[str(element.get('class'))] = {
                    'tag': element.name,
                    'attributes': element.attrs,
                    'content_type': self._identify_content_type(element)
                }
                
        return structure
        
    def _identify_content_type(self, element):
        """콘텐츠 타입 식별"""
        # 텍스트 비율 계산
        text_ratio = len(element.get_text(strip=True)) / len(str(element))
        
        # 이미지 수 계산
        image_count = len(element.find_all('img'))
        
        if text_ratio > 0.7:
            return 'text'
        elif image_count > 0:
            return 'mixed'
        else:
            return 'unknown'
```

## 12. 보안 및 우회 기술 고도화

### 12.1 고급 브라우저 자동화 우회

```python
class AdvancedBrowserAutomation:
    def __init__(self):
        self.driver = None
        self.stealth_patches = []
        
    def apply_stealth_patch(self):
        """스텔스 패치 적용"""
        patches = [
            self._patch_navigator(),
            self._patch_webdriver(),
            self._patch_chrome_runtime(),
            self._patch_permissions()
        ]
        
        for patch in patches:
            self.driver.execute_script(patch)
            
    def _patch_navigator(self):
        """Navigator 객체 패치"""
        return """
        const originalNavigator = window.navigator;
        const navigator_properties = {};
        
        for (const prop in originalNavigator) {
            if (prop !== 'webdriver') {
                navigator_properties[prop] = {
                    get: () => originalNavigator[prop]
                };
            }
        }
        
        navigator_properties['webdriver'] = {
            get: () => undefined
        };
        
        Object.defineProperties(
            window.navigator,
            navigator_properties
        );
        """
        
    def _patch_permissions(self):
        """Permissions API 패치"""
        return """
        const originalQuery = window.navigator.permissions.query;
        window.navigator.permissions.query = (parameters) => (
            parameters.name === 'notifications' ?
            Promise.resolve({ state: Notification.permission }) :
            originalQuery(parameters)
        );
        """
```

### 12.2 네트워크 트래픽 마스킹

```python
class NetworkTrafficMasker:
    def __init__(self):
        self.proxy_rotator = ProxyRotator()
        self.traffic_patterns = []
        
    def add_traffic_pattern(self, pattern):
        """트래픽 패턴 추가"""
        self.traffic_patterns.append(pattern)
        
    def generate_natural_traffic(self):
        """자연스러운 트래픽 생성"""
        pattern = random.choice(self.traffic_patterns)
        
        # 패턴에 따른 요청 생성
        requests = []
        for step in pattern:
            request = {
                'url': step['url'],
                'method': step['method'],
                'headers': self._generate_headers(),
                'timing': self._calculate_timing()
            }
            requests.append(request)
            
        return requests
        
    def _generate_headers(self):
        """동적 헤더 생성"""
        return {
            'User-Agent': self._get_random_user_agent(),
            'Accept-Language': self._get_random_language(),
            'Accept': self._get_random_accept_header(),
            'Connection': random.choice(['keep-alive', 'close']),
            'Cache-Control': random.choice(['no-cache', 'max-age=0'])
        }
        
    def _calculate_timing(self):
        """자연스러운 타이밍 계산"""
        # 가우시안 분포를 사용한 지연 시간 계산
        base_delay = random.gauss(2, 0.5)
        jitter = random.uniform(-0.1, 0.1)
        return max(0.1, base_delay + jitter)
```

## 13. 최종 실행 예제

```python
async def main():
    # 시스템 초기화
    crawler = IntegratedCrawlingSystem()
    logger = CrawlerLogger('MainSystem')
    monitor = PerformanceMonitor()
    
    try:
        # 크롤링 대상 URL 목록
        urls = [
            "https://example.com/page1",
            "https://example.com/page2",
            "https://example.com/page3"
        ]
        
        # 성능 모니터링 시작
        monitor.start_monitoring('main_task')
        
        # 크롤링 실행
        results = await crawler.run_crawling_task(urls)
        
        # 결과 처리
        if results:
            logger.logger.info(f"Successfully crawled {len(results)} pages")
            
            # 결과 저장
            save_results(results)
            
        # 성능 통계 출력
        stats = monitor.get_statistics('main_task')
        logger.logger.info(f"Performance statistics: {stats}")
        
    except Exception as e:
        logger.logger.error(f"Main task failed: {str(e)}")
        
    finally:
        # 리소스 정리
        cleanup_resources()

if __name__ == "__main__":
    asyncio.run(main())
```

## 주의사항 및 모범 사례

1. **웹사이트 리소스 존중**
   - robots.txt 규칙 준수
   - 적절한 요청 간격 유지
   - 과도한 부하 방지

2. **오류 처리 및 복구**
   - 모든 예외 상황 처리
   - 자동 재시도 메커니즘
   - 로깅 및 모니터링

3. **성능 최적화**
   - 비동기 작업 활용
   - 리소스 사용 최적화
   - 캐싱 전략 구현

4. **보안 고려사항**
   - IP 차단 방지
   - 봇 감지 우회
   - 데이터 보안 유지

이 가이드를 통해 안정적이고 효율적인 웹 크롤링 시스템을 구축할 수 있습니다. 특히 봇 감지 우회와 관련된 기술들을 적절히 활용하면서도, 웹사이트의 정책을 존중하는 균형 잡힌 접근이 중요합니다. 우회를 위한 랜덤 동작
            simulator.random_mouse_movement()
            simulator.random_scroll()
            
            # 페이지 크롤링
            content = crawler.get_page(url)
            
            # 데이터 처리 및 저장
            cleaned_data = DataCleaner.clean_text(content)
            data_handler.save_item({
                'url': url,
                'content': cleaned_data,
                'timestamp': datetime.now()
            })
            
    finally:
        crawler.driver.quit()
```

### 주의사항

1. **웹사이트 정책 준수**
   - robots.txt 규칙 확인
   - 크롤링 간격 조절
   - 서버 부하 고려

2. **에러 처리**
   - 네트워크 오류 대비
   - 타임아웃 설정
   - 재시도 메커니즘 구현
   - 예외 상황 로깅

3. **보안 고려사항**
   - 민감한 정보 암호화
   - 접근 권한 관리
   - IP 차단 방지

## 8. 고급 보안 및 우회 기법

### 8.1 브라우저 프로필 관리

```python
class BrowserProfileManager:
    def __init__(self):
        self.profiles_dir = "browser_profiles"
        os.makedirs(self.profiles_dir, exist_ok=True)
        
    def create_profile(self, profile_name):
        """새로운 브라우저 프로필 생성"""
        profile_path = os.path.join(self.profiles_dir, profile_name)
        
        options = Options()
        options.add_argument(f'--user-data-dir={profile_path}')
        options.add_argument('--profile-directory=Default')
        
        # 기본 설정 추가
        prefs = {
            'profile.default_content_setting_values.notifications': 2,
            'profile.password_manager_enabled': False,
            'profile.managed_default_content_settings.images': 2
        }
        options.add_experimental_option('prefs', prefs)
        
        return options
        
    def rotate_profile(self):
        """프로필 로테이션"""
        profiles = os.listdir(self.profiles_dir)
        return random.choice(profiles) if profiles else 'default'
```

### 8.2 고급 봇 감지 우회

```python
class AdvancedAntiDetection:
    def __init__(self, driver):
        self.driver = driver
        
    def bypass_selenium_detection(self):
        """Selenium 감지 우회"""
        # navigator.webdriver 속성 제거
        self.driver.execute_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
        """)
        
        # 자동화 관련 속성 제거
        self.driver.execute_script("""
            delete window.cdc_adoQpoasnfa76pfcZLmcfl_Array;
            delete window.cdc_adoQpoasnfa76pfcZLmcfl_Promise;
            delete window.cdc_adoQpoasnfa76pfcZLmcfl_Symbol;
        """)
        
    def emulate_human_timing(self):
        """인간다운 타이밍 패턴 생성"""
        def random_delay():
            # 가우시안 분포를 사용한 자연스러운 지연 시간
            mean_delay = 2
            std_dev = 0.5
            delay = random.gauss(mean_delay, std_dev)
            return max(0.1, delay)
            
        return random_delay()
        
    def randomize_mouse_movements(self):
        """자연스러운 마우스 움직임 생성"""
        def bezier_curve(p0, p1, p2, t):
            # 베지어 곡선을 사용한 자연스러운 마우스 움직임
            x = (1-t)**2 * p0[0] + 2*(1-t)*t * p1[0] + t**2 * p2[0]
            y = (1-t)**2 * p0[1] + 2*(1-t)*t * p1[1] + t**2 * p2[1]
            return (x, y)
            
        # 시작점과 끝점 설정
        start = (0, 0)
        end = (random.randint(100, 500), random.randint(100, 500))
        
        # 제어점 설정
        control = (random.randint(min(start[0], end[0]), max(start[0], end[0])),
                  random.randint(min(start[1], end[1]), max(start[1], end[1])))
                  
        # 곡선을 따라 움직임
        points = [bezier_curve(start, control, end, t/100) for t in range(101)]
        
        actions = ActionChains(self.driver)
        for point in points:
            actions.move_by_offset(*point)
            actions.pause(random.uniform(0.001, 0.003))
        
        actions.perform()
```

### 8.3 네트워크 트래픽 제어

```python
class NetworkTrafficController:
    def __init__(self, driver):
        self.driver = driver
        
    def set_request_interceptor(self):
        """요청 인터셉터 설정"""
        def interceptor(request):
            # 불필요한 리소스 차단
            if request.resource_type in ['image', 'media', 'font']:
                request.abort()
            else:
                request.continue_()
                
        self.driver.request_interceptor = interceptor
        
    def modify_headers(self):
        """헤더 수정"""
        def header_modifier(request):
            # 기존 헤더 수정
            headers = request.headers
            headers['Accept-Language'] = random.choice([
                'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
                'en-US,en;q=0.9,ko;q=0.8',
                'ja-JP,ja;q=0.9,en-US;q=0.8,en;q=0.7'
            ])
            request.headers = headers
            
        self.driver.header_modifier = header_modifier
```

### 8.4 고급 캡차 처리

```python
class AdvancedCaptchaSolver:
    def __init__(self, api_key=None):
        self.api_key = api_key
        self.audio_to_text = None  # 음성인식 엔진 초기화
        
    async def solve_audio_captcha(self, audio_url):
        """음성 캡차 해결"""
        try:
            # 음성 파일 다운로드
            async with aiohttp.ClientSession() as session:
                async with session.get(audio_url) as response:
                    audio_content = await response.read()
                    
            # 음성을 텍스트로 변환
            text = self.audio_to_text.recognize(audio_content)
            return text
            
        except Exception as e:
            print(f"음성 캡차 해결 실패: {str(e)}")
            return None
            
    def solve_puzzle_captcha(self, image_path):
        """퍼즐 캡차 해결"""
        try:
            # 이미지 로드
            image = cv2.imread(image_path)
            
            # 퍼즐 조각 찾기
            edges = cv2.Canny(image, 100, 200)
            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            # 퍼즐 조각 매칭
            # ... 퍼즐 매칭 로직 ...
            
            return {'x': x_offset, 'y': y_offset}
            
        except Exception as e:
            print(f"퍼즐 캡차 해결 실패: {str(e)}")
            return None
```

## 9. 고급 데이터 처리

### 9.1 병렬 처리 최적화

```python
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor

class ParallelProcessor:
    def __init__(self, max_workers=None):
        self.thread_executor = ThreadPoolExecutor(max_workers=max_workers)
        self.process_executor = ProcessPoolExecutor(max_workers=max_workers)
        
    def process_data_parallel(self, items, processor_func):
        """데이터 병렬 처리"""
        # CPU 바운드 작업은 ProcessPoolExecutor 사용
        if self._is_cpu_bound(processor_func):
            executor = self.process_executor
        else:
            # I/O 바운드 작업은 ThreadPoolExecutor 사용
            executor = self.thread_executor
            
        results = list(executor.map(processor_func, items))
        return results
        
    @staticmethod
    def _is_cpu_bound(func):
        """작업이 CPU 바운드인지 판단"""
        # 함수의 특성을 분석하여 CPU/IO 바운드 판단
        # ... 판단 로직 ...
        return True  # 예시
```

### 9.2 스트리밍 데이터 처리

```python
class StreamProcessor:
    def __init__(self, chunk_size=1000):
        self.chunk_size = chunk_size
        self.buffer = []
        
    async def process_stream(self, data_stream, processor_func):
        """스트리밍 데이터 처리"""
        async for item in data_stream:
            self.buffer.append(item)
            
            if len(self.buffer) >= self.chunk_size:
                await self._process_buffer(processor_func)
                
        # 남은 데이터 처리
        if self.buffer:
            await self._process_buffer(processor_func)
            
    async def _process_buffer(self, processor_func):
        """버퍼 데이터 처리"""
        try:
            results = await processor_func(self.buffer)
            self.buffer = []
            return results
        except Exception as e:
            print(f"버퍼 처리 실패: {str(e)}")
            return None
```

### 9.3 데이터 검증 및 정제

```python
class DataValidator:
    def __init__(self):
        self.schema = None
        self.rules = []
        
    def add_validation_rule(self, field, rule_func):
        """검증 규칙 추가"""
        self.rules.append((field, rule_func))
        
    def validate_data(self, data):
        """데이터 검증"""
        errors = []
        
        for field, rule_func in self.rules:
            if field in data:
                try:
                    if not rule_func(data[field]):
                        errors.append(f"Validation failed for {field}")
                except Exception as e:
                    errors.append(f"Error validating {field}: {str(e)}")
                    
        return len(errors) == 0, errors

class DataCleaner:
    @staticmethod
    def clean_html(text):
        """HTML 태그 제거"""
        soup = BeautifulSoup(text, 'html.parser')
        return soup.get_text()
        
    @staticmethod
    def normalize_whitespace(text):
        """공백 정규화"""
        return ' '.join(text.split())
        
    @staticmethod
    def remove_special_chars(text, keep_chars=''):
        """특수문자 제거"""
        pattern = f'[^\\w\\s{re.escape(keep_chars)}]'
        return re.sub(pattern, '', text)
```

## 10. 배포 및 모니터링

### 10.1 로깅 시스템

```python
import logging
from logging.handlers import RotatingFileHandler

class CrawlerLogger:
    def __init__(self, name):
        self.logger = logging.getLogger(name)
        self.setup_logger()
        
    def setup_logger(self):
        """로거 설정"""
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
        # 파일 핸들러 설정
        file_handler = RotatingFileHandler(
            'crawler.log',
            maxBytes=10000000,  # 10MB
            backupCount=5
        )
        file_handler.setFormatter(formatter)
        
        # 스트림 핸들러 설정
        stream_handler = logging.StreamHandler()
        stream_handler.setFormatter(formatter)
        
        self.logger.addHandler(file_handler)
        self.logger.addHandler(stream_handler)
        self.logger.setLevel(logging.INFO)
```

### 10.2 성능 모니터링

```python
class PerformanceMonitor:
    def __init__(self):
        self.metrics = {}
        
    def start_monitoring(self, task_name):
        """작업 모니터링 시작"""
        self.metrics[task_name] = {
            'start_time': time.time(),
            'success_count': 0,
            'error_count': 0,
            'response_times': []
        }
        
    def record_metric(self, task_name, success=True, response_time=None):
        """메트릭 기록"""
        if task_name in self.metrics:
            if success:
                self.metrics[task_name]['success_count'] += 1
            else:
                self.metrics[task_name]['error_count'] += 1
                
            if response_time:
                self.metrics[task_name]['response_times'].append(response_time)
                
    def get_statistics(self, task_name):
        """통계 정보 반환"""
        if task_name in self.metrics:
            stats = self.metrics[task_name]
            total_time = time.time() - stats['start_time']
            
            return {
                'total_time': total_time,
                'success_rate': stats['success_count'] / (stats['success_count'] + stats['error_count']),
                'average_response_time': sum(stats['response_times']) / len(stats['response_times']) if stats['response_times'] else 0,
                'requests_per_second': (stats['success_count'] + stats['error_count']) / total_time
            }
        return None
```

## 실제 사용 예제

### 전체 시스템 통합 예제

"""
통합 크롤링 시스템 완성본
- 비동기 처리
- 캡차 처리
- 봇 감지 우회
- 데이터 처리
- 오류 처리
- 모니터링
"""

'''python
import asyncio
import time
import random
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from selenium.webdriver import ActionChains

class IntegratedCrawlingSystem:
    def __init__(self):
        self.browser_manager = BrowserProfileManager()
        self.anti_detection = None
        self.logger = CrawlerLogger('IntegratedCrawler')
        self.monitor = PerformanceMonitor()
        self.data_handler = DatabaseHandler()
        self.captcha_solver = AdvancedCaptchaSolver()
        self.proxy_manager = ProxyManager()
        
    async def initialize_system(self):
        """시스템 초기화"""
        try:
            # 프록시 풀 초기화
            await self.proxy_manager.initialize_proxy_pool()
            
            # 브라우저 프로필 준비
            await self.browser_manager.prepare_profiles()
            
            # 데이터베이스 연결 확인
            await self.data_handler.verify_connection()
            
            self.logger.logger.info("System initialized successfully")
            return True
            
        except Exception as e:
            self.logger.logger.error(f"System initialization failed: {str(e)}")
            return False
            
    async def run_crawling_task(self, urls, retry_count=3):
        """크롤링 작업 실행"""
        self.monitor.start_monitoring('crawling_task')
        results = []
        driver = None
        
        try:
            # 브라우저 설정
            options = await self.browser_manager.create_profile('main')
            driver = await self._setup_driver(options)
            
            # 각 URL 처리
            for url in urls:
                try:
                    self.logger.logger.info(f"Processing URL: {url}")
                    result = await self._process_single_url(driver, url, retry_count)
                    if result:
                        results.append(result)
                        
                except Exception as e:
                    self.logger.logger.error(f"Error processing URL {url}: {str(e)}")
                    continue
                    
            return results
            
        except Exception as e:
            self.logger.logger.error(f"Critical error in crawling task: {str(e)}")
            raise
            
        finally:
            if driver:
                await self._cleanup(driver)
                
    async def _setup_driver(self, options):
        """웹드라이버 설정"""
        driver = webdriver.Chrome(options=options)
        
        # 봇 감지 우회 설정
        self.anti_detection = AdvancedAntiDetection(driver)
        await self.anti_detection.setup_advanced_evasion()
        
        # 네트워크 트래픽 제어
        network_controller = NetworkTrafficController(driver)
        await network_controller.setup_traffic_control()
        
        return driver
        
    async def _process_single_url(self, driver, url, retry_count):
        """단일 URL 처리"""
        for attempt in range(retry_count):
            try:
                # 프록시 로테이션 (첫 시도 제외)
                if attempt > 0:
                    await self.proxy_manager.rotate_proxy(driver)
                    
                # 인간다운 행동 시뮬레이션
                await self._simulate_human_behavior(driver)
                
                # 페이지 크롤링
                start_time = time.time()
                content = await self._crawl_page(driver, url)
                response_time = time.time() - start_time
                
                if content:
                    # 캡차 확인 및 처리
                    if await self._check_for_captcha(driver):
                        await self._handle_captcha(driver)
                    
                    # 데이터 처리
                    processed_data = await self._process_data(content, url)
                    
                    # 성능 메트릭 기록
                    self.monitor.record_metric('crawling_task', True, response_time)
                    
                    return processed_data
                    
            except CaptchaException as ce:
                self.logger.logger.warning(f"Captcha detected on {url}, attempt {attempt + 1}")
                if attempt == retry_count - 1:
                    raise ce
                continue
                
            except Exception as e:
                self.logger.logger.error(f"Error on attempt {attempt + 1} for {url}: {str(e)}")
                if attempt == retry_count - 1:
                    self.monitor.record_metric('crawling_task', False)
                    raise e
                
                # 지수 백오프
                await asyncio.sleep(2 ** attempt)
                
        return None
        
    async def _simulate_human_behavior(self, driver):
        """인간다운 행동 시뮬레이션"""
        try:
            # 랜덤 스크롤
            scroll_amount = random.randint(100, 500)
            driver.execute_script(f"window.scrollBy(0, {scroll_amount});")
            
            # 랜덤 마우스 움직임
            actions = ActionChains(driver)
            x_offset = random.randint(-100, 100)
            y_offset = random.randint(-100, 100)
            actions.move_by_offset(x_offset, y_offset)
            actions.perform()
            
            # 랜덤 대기
            await asyncio.sleep(random.uniform(1, 3))
            
        except Exception as e:
            self.logger.logger.error(f"Error in human behavior simulation: {str(e)}")
            
    async def _crawl_page(self, driver, url):
        """페이지 크롤링"""
        try:
            # 페이지 로드
            driver.get(url)
            
            # 동적 콘텐츠 대기
            dynamic_handler = DynamicContentHandler(driver)
            await dynamic_handler.wait_for_dynamic_content()
            
            # 무한 스크롤 처리
            if await self._has_infinite_scroll(driver):
                await dynamic_handler.handle_infinite_scroll()
            
            # AJAX 요청 완료 대기
            ajax_handler = AjaxRequestHandler(driver)
            await ajax_handler.wait_for_ajax_completion()
            
            return driver.page_source
            
        except TimeoutException:
            self.logger.logger.error(f"Timeout while loading {url}")
            return None
            
        except Exception as e:
            self.logger.logger.error(f"Error in crawl_page: {str(e)}")
            return None
            
    async def _process_data(self, content, url):
        """데이터 처리"""
        try:
            # 데이터 정제
            cleaned_data = await self._clean_data(content)
            
            # 데이터 검증
            if not await self._validate_data(cleaned_data):
                return None
            
            # 데이터 변환
            transformed_data = await self._transform_data(cleaned_data, url)
            
            # 데이터 저장
            if await self.data_handler.store_data(transformed_data):
                self.logger.logger.info(f"Data successfully stored for {url}")
                return transformed_data
                
            return None
            
        except Exception as e:
            self.logger.logger.error(f"Error processing data: {str(e)}")
            return None
            
    async def _clean_data(self, content):
        """데이터 정제"""
        cleaner = DataCleaner()
        return await cleaner.clean_content(content)
        
    async def _validate_data(self, data):
        """데이터 검증"""
        validator = DataValidator()
        is_valid, errors = await validator.validate_data(data)
        
        if not is_valid:
            self.logger.logger.warning(f"Data validation errors: {errors}")
            
        return is_valid
        
    async def _transform_data(self, data, url):
        """데이터 변환"""
        return {
            'url': url,
            'content': data,
            'timestamp': datetime.now(),
            'metadata': {
                'crawler_version': '1.0',
                'processing_time': time.time()
            }
        }
        
    async def _check_for_captcha(self, driver):
        """캡차 확인"""
        captcha_indicators = [
            '//iframe[contains(@src, "recaptcha")]',
            '//iframe[contains(@src, "hcaptcha")]',
            '//*[contains(text(), "captcha")]'
        ]
        
        for indicator in captcha_indicators:
            try:
                element = driver.find_element(By.XPATH, indicator)
                if element.is_displayed():
                    return True
            except NoSuchElementException:
                continue
                
        return False
        
    async def _handle_captcha(self, driver):
        """캡차 처리"""
        try:
            captcha_type = await self._detect_captcha_type(driver)
            
            if captcha_type == 'recaptcha':
                solved = await self.captcha_solver.solve_recaptcha(driver)
            elif captcha_type == 'hcaptcha':
                solved = await self.captcha_solver.solve_hcaptcha(driver)
            else:
                solved = await self.captcha_solver.solve_image_captcha(driver)
                
            if not solved:
                raise CaptchaException("Failed to solve captcha")
                
        except Exception as e:
            raise CaptchaException(f"Error handling captcha: {str(e)}")
            
    async def _detect_captcha_type(self, driver):
        """캡차 유형 감지"""
        if driver.find_elements(By.XPATH, '//iframe[contains(@src, "recaptcha")]'):
            return 'recaptcha'
        elif driver.find_elements(By.XPATH, '//iframe[contains(@src, "hcaptcha")]'):
            return 'hcaptcha'
        else:
            return 'image'
            
    async def _has_infinite_scroll(self, driver):
        """무한 스크롤 확인"""
        try:
            last_height = driver.execute_script("return document.body.scrollHeight")
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            await asyncio.sleep(2)
            new_height = driver.execute_script("return document.body.scrollHeight")
            
            return new_height > last_height
            
        except Exception:
            return False
            
    async def _cleanup(self, driver):
        """리소스 정리"""
        try:
            if driver:
                driver.quit()
            
            await self.browser_manager.cleanup_profiles()
            await self.data_handler.close_connection()
            
            self.logger.logger.info("Cleanup completed successfully")
            
        except Exception as e:
            self.logger.logger.error(f"Error during cleanup: {str(e)}")


# 사용 예제
async def main():
    # 크롤링 시스템 초기화
    crawler = IntegratedCrawlingSystem()
    
    # 크롤링할 URL 목록
    urls = [
        "https://example.com/page1",
        "https://example.com/page2",
        "https://example.com/page3"
    ]
    
    try:
        # 시스템 초기화
        if not await crawler.initialize_system():
            raise Exception("System initialization failed")
            
        # 크롤링 실행
        results = await crawler.run_crawling_task(urls)
        
        # 결과 처리
        if results:
            print(f"Successfully crawled {len(results)} pages")
            for result in results:
                print(f"Data from {result['url']}:")
                print(f"- Timestamp: {result['timestamp']}")
                print(f"- Content length: {len(result['content'])}")
                print("---")
                
    except Exception as e:
        print(f"Error in main: {str(e)}")
        
    finally:
        # 시스템 정리
        await crawler._cleanup(None)

# 실행
if __name__ == "__main__":
    asyncio.run(main())
'''